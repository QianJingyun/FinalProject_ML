{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a257951-25c9-4781-b45a-09c86937b616",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch' has no attribute 'no_grad'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torchvision/__init__.py:6\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodulefinder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Module\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datasets, io, models, ops, transforms, utils\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextension\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _HAS_OPS\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torchvision/datasets/__init__.py:1\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_optical_flow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FlyingChairs, FlyingThings3D, HD1K, KittiFlow, Sintel\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_stereo_matching\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      3\u001b[0m     CarlaStereo,\n\u001b[1;32m      4\u001b[0m     CREStereo,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m     SintelStereo,\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcaltech\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Caltech101, Caltech256\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torchvision/datasets/_optical_flow.py:12\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _read_png_16\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _read_pfm, verify_str_arg\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VisionDataset\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torchvision/io/__init__.py:5\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Dict, Iterator\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _log_api_usage_once\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_load_gpu_decoder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _HAS_GPU_VIDEO_DECODER\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torchvision/utils.py:23\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image, ImageColor, ImageDraw, ImageFont\n\u001b[1;32m     13\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmake_grid\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msave_image\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflow_to_image\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     20\u001b[0m ]\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mno_grad\u001b[49m()\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmake_grid\u001b[39m(\n\u001b[1;32m     25\u001b[0m     tensor: Union[torch\u001b[38;5;241m.\u001b[39mTensor, List[torch\u001b[38;5;241m.\u001b[39mTensor]],\n\u001b[1;32m     26\u001b[0m     nrow: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m,\n\u001b[1;32m     27\u001b[0m     padding: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m     28\u001b[0m     normalize: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     29\u001b[0m     value_range: Optional[Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     30\u001b[0m     scale_each: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     31\u001b[0m     pad_value: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m,\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m     33\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124;03m    Make a grid of images.\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;124;03m        grid (Tensor): the tensor containing grid of images.\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_tracing():\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch' has no attribute 'no_grad'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ffce8195-3b71-4d5e-8e84-c44245d326dd",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch' has no attribute 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mNIH_CXR_Dataset\u001b[39;00m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, data_dir, label_dir, split):\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_dir \u001b[38;5;241m=\u001b[39m data_dir\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch' has no attribute 'utils'"
     ]
    }
   ],
   "source": [
    "class NIH_CXR_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_dir, label_dir, split):\n",
    "        self.data_dir = data_dir\n",
    "        self.split = split\n",
    "\n",
    "        self.CLASSES = [\n",
    "            'No Finding', 'Infiltration', 'Atelectasis', 'Effusion', 'Nodule',\n",
    "            'Mass', 'Pneumothorax', 'Consolidation', 'Pleural_Thickening',\n",
    "            'Cardiomegaly', 'Fibrosis', 'Edema', 'Tortuous Aorta', 'Emphysema',\n",
    "            'Pneumonia', 'Calcification of the Aorta', 'Pneumoperitoneum', 'Hernia',\n",
    "            'Subcutaneous Emphysema', 'Pneumomediastinum'\n",
    "        ]\n",
    "\n",
    "        self.label_df = pd.read_csv(os.path.join(label_dir, f'nih-lt_single-label_{split}.csv'))\n",
    "\n",
    "        self.img_paths = self.label_df['id'].apply(lambda x: os.path.join(data_dir, x)).values.tolist()\n",
    "        self.labels = self.label_df[self.CLASSES].idxmax(axis=1).apply(lambda x: self.CLASSES.index(x)).values\n",
    "\n",
    "        self.cls_num_list = self.label_df[self.CLASSES].sum(0).values.tolist()\n",
    "\n",
    "        if self.split == 'train':\n",
    "            self.transform = torchvision.transforms.Compose([\n",
    "                torchvision.transforms.ToPILImage(),\n",
    "                torchvision.transforms.RandomHorizontalFlip(),\n",
    "                torchvision.transforms.RandomRotation(15),\n",
    "                torchvision.transforms.ToTensor(),\n",
    "                torchvision.transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225) )\n",
    "            ])\n",
    "        else:\n",
    "            self.transform = torchvision.transforms.Compose([\n",
    "                torchvision.transforms.ToPILImage(),\n",
    "                torchvision.transforms.ToTensor(),\n",
    "                torchvision.transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225) )\n",
    "            ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = cv2.imread(self.img_paths[idx])\n",
    "        x = cv2.resize(x, (256, 256), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "        x = self.transform(x)\n",
    "\n",
    "        y = np.array(self.labels[idx])\n",
    "\n",
    "        return x.float(), torch.from_numpy(y).long()\n",
    "\n",
    "class MIMIC_CXR_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_dir, label_dir, split):\n",
    "        self.split = split\n",
    "\n",
    "        self.CLASSES = [\n",
    "            'No Finding', 'Lung Opacity', 'Cardiomegaly', 'Atelectasis',\n",
    "            'Pleural Effusion', 'Support Devices', 'Edema', 'Pneumonia',\n",
    "            'Pneumothorax', 'Lung Lesion', 'Fracture', 'Enlarged Cardiomediastinum',\n",
    "            'Consolidation', 'Pleural Other', 'Calcification of the Aorta',\n",
    "            'Tortuous Aorta', 'Pneumoperitoneum', 'Subcutaneous Emphysema',\n",
    "            'Pneumomediastinum'\n",
    "        ]\n",
    "\n",
    "        self.label_df = pd.read_csv(os.path.join(label_dir, f'mimic-lt_single-label_{split}.csv'))\n",
    "\n",
    "        self.img_paths = self.label_df['path'].apply(lambda x: os.path.join(data_dir, x)).values.tolist()\n",
    "        self.labels = self.label_df[self.CLASSES].idxmax(axis=1).apply(lambda x: self.CLASSES.index(x)).values\n",
    "\n",
    "        self.cls_num_list = self.label_df[self.CLASSES].sum(0).values.tolist()\n",
    "\n",
    "        if self.split == 'train':\n",
    "            self.transform = torchvision.transforms.Compose([\n",
    "                torchvision.transforms.ToPILImage(),\n",
    "                torchvision.transforms.RandomHorizontalFlip(),\n",
    "                torchvision.transforms.RandomRotation(15),\n",
    "                torchvision.transforms.ToTensor(),\n",
    "                torchvision.transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225) )\n",
    "            ])\n",
    "        else:\n",
    "            self.transform = torchvision.transforms.Compose([\n",
    "                torchvision.transforms.ToPILImage(),\n",
    "                torchvision.transforms.ToTensor(),\n",
    "                torchvision.transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225) )\n",
    "            ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = cv2.imread(self.img_paths[idx])\n",
    "        x = cv2.resize(x, (256, 256), interpolation=cv2.INTER_AREA)\n",
    "        x = self.transform(x)\n",
    "\n",
    "        y = np.array(self.labels[idx])\n",
    "\n",
    "        return x.float(), torch.from_numpy(y).long()\n",
    "\n",
    "## CREDIT TO https://github.com/agaldran/balanced_mixup ##\n",
    "\n",
    "# pytorch-wrapping-multi-dataloaders/blob/master/wrapping_multi_dataloaders.py\n",
    "class ComboIter(object):\n",
    "    \"\"\"An iterator.\"\"\"\n",
    "    def __init__(self, my_loader):\n",
    "        self.my_loader = my_loader\n",
    "        self.loader_iters = [iter(loader) for loader in self.my_loader.loaders]\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        # When the shortest loader (the one with minimum number of batches)\n",
    "        # terminates, this iterator will terminates.\n",
    "        # The `StopIteration` raised inside that shortest loader's `__next__`\n",
    "        # method will in turn gets out of this `__next__` method.\n",
    "        batches = [loader_iter.next() for loader_iter in self.loader_iters]\n",
    "        return self.my_loader.combine_batch(batches)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.my_loader)\n",
    "\n",
    "class ComboLoader(object):\n",
    "    \"\"\"This class wraps several pytorch DataLoader objects, allowing each time\n",
    "    taking a batch from each of them and then combining these several batches\n",
    "    into one. This class mimics the `for batch in loader:` interface of\n",
    "    pytorch `DataLoader`.\n",
    "    Args:\n",
    "    loaders: a list or tuple of pytorch DataLoader objects\n",
    "    \"\"\"\n",
    "    def __init__(self, loaders):\n",
    "        self.loaders = loaders\n",
    "\n",
    "    def __iter__(self):\n",
    "        return ComboIter(self)\n",
    "\n",
    "    def __len__(self):\n",
    "        return min([len(loader) for loader in self.loaders])\n",
    "\n",
    "    # Customize the behavior of combining batches here.\n",
    "    def combine_batch(self, batches):\n",
    "        return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc370766-e2cf-4038-a9ab-da30d0a2e685",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1dcd803-4926-4eaf-984e-4038254b73d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
